{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GloVe.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMh5OCM2pUpTsnjjvuB2KSw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benza613/Sentiment-Analysis-using-Deep-Learning/blob/master/notebooks/GloVe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QDpxvKUL_d7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gVqnXEsOmb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd gdrive/My\\ Drive/SNLP"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rEMwgR5Ov1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd Sentiment-Analysis-using-Deep-Learning/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1vD3rrlRt0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#used to unzip the glove zip folder\n",
        "#import zipfile\n",
        "#with zipfile.ZipFile('glove.840B.300d.zip', 'r') as zip_ref:\n",
        " #   zip_ref.extractall('glove.840B.300d')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNFlsOhiATny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Train_ZipCSVFileName = 'data/Train_2000_Apr-03-2020_06-46.csv'\n",
        "Test_ZipCSVFileName = 'data/Test_2000_Apr-03-2020_06-46.csv'\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "df_train = pd.read_csv(Train_ZipCSVFileName)\n",
        "df_train.info()\n",
        "\n",
        "df_test = pd.read_csv(Test_ZipCSVFileName)\n",
        "df_test.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5sG1KQBTJXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6X6JnOpAtFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get names of indexes for which column overall has value 3\n",
        "index_neutrals_train = df_train[ df_train['overall'] == 3 ].index\n",
        "index_neutrals_test = df_test[ df_test['overall'] == 3 ].index\n",
        " \n",
        "# Delete these row indexes from dataFrame\n",
        "df_train.drop(index_neutrals_train , inplace=True)\n",
        "df_test.drop(index_neutrals_test , inplace=True)\n",
        "\n",
        "df_train.loc[(df_train.overall == 1),'overall']= 1\n",
        "df_train.loc[(df_train.overall == 2),'overall']= 1\n",
        "df_train.loc[(df_train.overall == 4),'overall']= 5\n",
        "df_train.loc[(df_train.overall == 5),'overall']= 5\n",
        "\n",
        "df_test.loc[(df_test.overall == 1),'overall']= 1\n",
        "df_test.loc[(df_test.overall == 2),'overall']= 1\n",
        "df_test.loc[(df_test.overall == 4),'overall']= 5\n",
        "df_test.loc[(df_test.overall == 5),'overall']= 5\n",
        "\n",
        "df_train['reviewText_len'].describe()\n",
        "# Since the mean average review size is around 145 chars and max is 400, I can safely set the max [summary + review] Text Limit to 400 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdFBDHkWA6y0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train['overall'].describe()\n",
        "df_test['overall'].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cnj7F8PA_dG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_VOCAB_SIZE = 50000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 200\n",
        "\n",
        "import uuid\n",
        "folderGUID = uuid.uuid4().hex\n",
        "#folderGUID=\"b3133d6101874b4ba9158c9e261b2daf\"\n",
        "# stupid shell way of converting variable to string \n",
        "print(folderGUID)\n",
        "\n",
        "!mkdir \"ModelResults/$folderGUID\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBSoDjOJBDjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "appdg2WIBGHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words= MAX_VOCAB_SIZE, filters='#$%&()*+<=>@[\\\\]^_`{|}~\\t\\n', lower=True)\n",
        "tokenizer.fit_on_texts(df_train['summary'] + ' DELIM '+df_train['reviewText'])\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyTfW3wGBI6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = tokenizer.texts_to_sequences(df_train['summary'].values + ' DELIM '+df_train['reviewText'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJYqnrZqBJ4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = pd.get_dummies(df_train['overall']).values\n",
        "print('Shape of label tensor:', Y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db5S4dbMLEUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_index = {}\n",
        "\n",
        "f = open('glove.6B/glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:],dtype='float32')\n",
        "    embedding_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('found word vecs: ',len(embedding_index))\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((len(word_index)+1,embedding_dim))\n",
        "embedding_matrix.shape\n",
        "for word,i in word_index.items():\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "#model\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Bidirectional,GlobalMaxPool1D,Conv1D\n",
        "from keras.layers import LSTM,Input,Dense,Dropout,Activation\n",
        "from keras.models import Model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr4L5lSsqQrY",
        "colab_type": "text"
      },
      "source": [
        "Below are the functions for recall, F1 and precissio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c4ijLeZirim",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oem2SpiaiOcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_layer = Embedding(len(word_index)+1,embedding_dim,weights=[embedding_matrix],input_length=MAX_SEQUENCE_LENGTH,trainable=False)\n",
        "inp = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "x = embedding_layer(inp)\n",
        "x = Bidirectional(LSTM(50,return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
        "x = GlobalMaxPool1D()(x)\n",
        "x = Dense(50,activation='relu')(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(2,activation='sigmoid')(x)\n",
        "model = Model(inputs=inp,outputs=x)\n",
        "\n",
        "#call the functions in the metrics \n",
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc',f1_m,precision_m, recall_m])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQCCsGDtL3i8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(X,Y,validation_split=0.3, epochs=2, verbose=0);\n",
        "\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(df_test['summary'].values + ' DELIM '+df_test['reviewText'].values)\n",
        "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X_test.shape)\n",
        "\n",
        "Y_Test = pd.get_dummies(df_test['overall']).values\n",
        "print('Shape of label tensor:', Y_Test.shape)\n",
        "\n",
        "loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, Y_Test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SYU1Jtqqk4A",
        "colab_type": "text"
      },
      "source": [
        "The values are printed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WHmy5sEjsIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"loss\",loss)\n",
        "print(\"accuracy\",accuracy)\n",
        "print(\"f1 score\",f1_score)\n",
        "print(\"precision\",precision)\n",
        "print(\"recall\",recall)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTwxf4G-VvAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filePath = \"ModelResults/\"+str(folderGUID)+\"/model.h5\"\n",
        "model.save(filePath)\n",
        "print(\"Saved model to disk : \"+ str(folderGUID))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSU9cgISV2OA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = tokenizer.texts_to_sequences(df_test['summary'].values + ' DELIM '+df_test['reviewText'].values)\n",
        "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X_test.shape)\n",
        "\n",
        "Y_Test = pd.get_dummies(df_test['overall']).values\n",
        "print('Shape of label tensor:', Y_Test.shape)\n",
        "\n",
        "accr = model.evaluate(X_test,Y_Test)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ysPzy5qV78u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test for single review \n",
        "new_review = ['I am a victim of identity theft and someone stole my identity and personal information to open up a Visa credit card account with Bank of America. The following Bank of America Visa credit card account do not belong to me : XXXX.']\n",
        "seq = tokenizer.texts_to_sequences(new_review)\n",
        "padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "pred = model.predict(padded)\n",
        "\n",
        "print(pred)\n",
        "print(np.argmax(pred))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}