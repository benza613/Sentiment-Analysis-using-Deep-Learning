{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "model_lstm.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGPgMSbNPpOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a notebook for LSTM prediction model \n",
        "\n",
        "# 1. Load the chosen dataset and try to see the vocab size \n",
        "# 2. Determine a MAX_VOCAB_SIZE incase you observe a vocab dict that is too large (choose the top/most frequent MAX_VOCAB_SIZE entries / Curse of dimensionality)\n",
        "# 3. Determine a MAX_SEQUENCE LENGTH to vectorize for each review (Note: From splitter usually reviews will be limited to 400 + summary )\n",
        "\n",
        "# DO NOT EDIT/ DELETE THIS BLOCK;\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoSwNmKToBLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qYW78oFoQnP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1fbbe393-a367-4947-ce1e-47bac9e90e28"
      },
      "source": [
        "# set working directory -> you must set the path into which you have uploaded the zipped file\n",
        "# this is required in the case of colab or local \n",
        "%cd /content/drive/My\\ Drive/SNLP\\ Project\n",
        "# %cd Source/repos/Sentiment-Analysis-using-Deep-Learning"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/SNLP Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q-BzQJ4obkb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "61d037f3-9f49-45d0-d181-44cf6622fbbc"
      },
      "source": [
        "# list content of drive - verify you are where you are supposed to be\n",
        "%ls"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " CleantData_Apr-03-2020_01-31.zip        'Project Ideas.gdoc'\n",
            " \u001b[0m\u001b[01;34mData_Balanced_20000_Apr-03-2020_06-52\u001b[0m/  'Project Proposal.gdoc'\n",
            " \u001b[01;34mData_Balanced_2000_Apr-03-2020_06-46\u001b[0m/   \u001b[01;34m'Report MetaData'\u001b[0m/\n",
            " Electronics_5.json.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9Kz7amjoljp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "178e6847-3685-4873-c0fb-8441174663c1"
      },
      "source": [
        "# load your choice of dataset here . Specify paths as folder_datestring/file_datestring.zip\n",
        "Train_ZipCSVFileName = 'Data_Balanced_2000_Apr-03-2020_06-46/Train_2000_Apr-03-2020_06-46.zip'\n",
        "Test_ZipCSVFileName = 'Data_Balanced_2000_Apr-03-2020_06-46/Test_2000_Apr-03-2020_06-46.zip'\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "df_train = pd.read_csv(Train_ZipCSVFileName)\n",
        "df_train.info()\n",
        "\n",
        "df_test = pd.read_csv(Test_ZipCSVFileName)\n",
        "df_test.info()\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 9 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   overall         10000 non-null  float64\n",
            " 1   reviewTime      10000 non-null  object \n",
            " 2   reviewerID      10000 non-null  object \n",
            " 3   asin            10000 non-null  object \n",
            " 4   reviewText      10000 non-null  object \n",
            " 5   summary         10000 non-null  object \n",
            " 6   unixReviewTime  10000 non-null  int64  \n",
            " 7   reviewText_len  10000 non-null  int64  \n",
            " 8   summary_len     10000 non-null  int64  \n",
            "dtypes: float64(1), int64(3), object(5)\n",
            "memory usage: 703.2+ KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 9 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   overall         1000 non-null   float64\n",
            " 1   reviewTime      1000 non-null   object \n",
            " 2   reviewerID      1000 non-null   object \n",
            " 3   asin            1000 non-null   object \n",
            " 4   reviewText      1000 non-null   object \n",
            " 5   summary         1000 non-null   object \n",
            " 6   unixReviewTime  1000 non-null   int64  \n",
            " 7   reviewText_len  1000 non-null   int64  \n",
            " 8   summary_len     1000 non-null   int64  \n",
            "dtypes: float64(1), int64(3), object(5)\n",
            "memory usage: 70.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEPUIWc2pYRT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "1227ddce-b823-4c56-b6cb-f66b8fd81c6f"
      },
      "source": [
        "df_train['reviewText_len'].describe()\n",
        "# Since the mean average review size is around 145 chars and max is 400, I can safely set the max [summary + review] Text Limit to 400 \n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    10000.000000\n",
              "mean       144.724600\n",
              "std        108.495885\n",
              "min          1.000000\n",
              "25%         47.000000\n",
              "50%        126.000000\n",
              "75%        222.000000\n",
              "max        399.000000\n",
              "Name: reviewText_len, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13adMhDFoQAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_VOCAB_SIZE = 50000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGBHgCmFr496",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDpJYm15Psh8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6a4e0869-4cf4-4400-eede-eddd4e70353b"
      },
      "source": [
        "tokenizer = Tokenizer(num_words= MAX_VOCAB_SIZE, filters='#$%&()*+<=>@[\\\\]^_`{|}~\\t\\n', lower=True)\n",
        "tokenizer.fit_on_texts(df_train['reviewText'])\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8707 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pjaMEuHVXnR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dc4b4c8e-0fba-4020-9f51-609c866a7f9b"
      },
      "source": [
        "X = tokenizer.texts_to_sequences(df_train['reviewText'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X.shape)\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (10000, 250)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jng1UISwsXxd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "97c8e2af-d2c6-4f27-8f62-b21b9180e38d"
      },
      "source": [
        "Y = pd.get_dummies(df_train['overall']).values\n",
        "print('Shape of label tensor:', Y.shape)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of label tensor: (10000, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzu3hiE2strx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_VOCAB_SIZE, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "#model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(200))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "history = model.fit(X, Y, epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x0o4hHdHzxe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "67a55fd9-b156-42b7-bc8f-31b2f9113264"
      },
      "source": [
        "\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(df_test['reviewText'].values)\n",
        "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X_test.shape)\n",
        "\n",
        "Y_Test = pd.get_dummies(df_test['overall']).values\n",
        "print('Shape of label tensor:', Y_Test.shape)\n",
        "\n",
        "accr = model.evaluate(X_test,Y_Test)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (1000, 250)\n",
            "Shape of label tensor: (1000, 5)\n",
            "32/32 [==============================] - 6s 196ms/step - loss: 2.4391 - accuracy: 0.4340\n",
            "Test set\n",
            "  Loss: 2.439\n",
            "  Accuracy: 0.434\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}