{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "model_lstm.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGPgMSbNPpOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a notebook for LSTM prediction model \n",
        "\n",
        "# 1. Load the chosen dataset and try to see the vocab size \n",
        "# 2. Determine a MAX_VOCAB_SIZE incase you observe a vocab dict that is too large (choose the top/most frequent MAX_VOCAB_SIZE entries / Curse of dimensionality)\n",
        "# 3. Determine a MAX_SEQUENCE LENGTH to vectorize for each review (Note: From splitter usually reviews will be limited to 400 + summary )\n",
        "\n",
        "# DO NOT EDIT/ DELETE THIS BLOCK;\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoSwNmKToBLT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "248fbff4-f98d-4abc-96de-d838f22de50b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qYW78oFoQnP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1e8d4e94-57bf-445d-ab77-12e1c66c693e"
      },
      "source": [
        "# set working directory -> you must set the path into which you have uploaded the zipped file\n",
        "# this is required in the case of colab or local \n",
        "%cd /content/drive/My\\ Drive/SNLP\\ Project\n",
        "# %cd Source/repos/Sentiment-Analysis-using-Deep-Learning"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/SNLP Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q-BzQJ4obkb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "4ec04da8-6010-4057-a943-849070e2da9e"
      },
      "source": [
        "# list content of drive - verify you are where you are supposed to be\n",
        "%ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " CleantData_Apr-03-2020_01-31.zip          \u001b[0m\u001b[01;34mLexical_analysis\u001b[0m/\n",
            " \u001b[01;34mData_Balanced_100000_Apr-04-2020_06-54\u001b[0m/   \u001b[01;34mModelResults\u001b[0m/\n",
            " \u001b[01;34mData_Balanced_20000_Apr-03-2020_06-52\u001b[0m/   'Project Ideas.gdoc'\n",
            " \u001b[01;34mData_Balanced_2000_Apr-03-2020_06-46\u001b[0m/    'Project Proposal.gdoc'\n",
            " Electronics_5.json.gz                    \u001b[01;34m'Report MetaData'\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9Kz7amjoljp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "2df5c0ad-80fa-4f26-9682-a89cc6ceaf5e"
      },
      "source": [
        "# load your choice of dataset here . Specify paths as folder_datestring/file_datestring.zip\n",
        "Train_ZipCSVFileName = 'Data_Balanced_20000_Apr-03-2020_06-52/Train_20000_Apr-03-2020_06-52.zip'\n",
        "Test_ZipCSVFileName = 'Data_Balanced_20000_Apr-03-2020_06-52/Test_20000_Apr-03-2020_06-52.zip'\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "df_train = pd.read_csv(Train_ZipCSVFileName)\n",
        "df_train.info()\n",
        "\n",
        "df_test = pd.read_csv(Test_ZipCSVFileName)\n",
        "df_test.info()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100000 entries, 0 to 99999\n",
            "Data columns (total 9 columns):\n",
            " #   Column          Non-Null Count   Dtype  \n",
            "---  ------          --------------   -----  \n",
            " 0   overall         100000 non-null  float64\n",
            " 1   reviewTime      100000 non-null  object \n",
            " 2   reviewerID      100000 non-null  object \n",
            " 3   asin            100000 non-null  object \n",
            " 4   reviewText      100000 non-null  object \n",
            " 5   summary         100000 non-null  object \n",
            " 6   unixReviewTime  100000 non-null  int64  \n",
            " 7   reviewText_len  100000 non-null  int64  \n",
            " 8   summary_len     100000 non-null  int64  \n",
            "dtypes: float64(1), int64(3), object(5)\n",
            "memory usage: 6.9+ MB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 9 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   overall         10000 non-null  float64\n",
            " 1   reviewTime      10000 non-null  object \n",
            " 2   reviewerID      10000 non-null  object \n",
            " 3   asin            10000 non-null  object \n",
            " 4   reviewText      10000 non-null  object \n",
            " 5   summary         10000 non-null  object \n",
            " 6   unixReviewTime  10000 non-null  int64  \n",
            " 7   reviewText_len  10000 non-null  int64  \n",
            " 8   summary_len     10000 non-null  int64  \n",
            "dtypes: float64(1), int64(3), object(5)\n",
            "memory usage: 703.2+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEPUIWc2pYRT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "e71990e0-5a05-4f6e-9da2-a0c4db535be1"
      },
      "source": [
        "\n",
        "# Get names of indexes for which column Age has value 30\n",
        "index_neutrals_train = df_train[ df_train['overall'] == 3 ].index\n",
        "index_neutrals_test = df_test[ df_test['overall'] == 3 ].index\n",
        " \n",
        "# Delete these row indexes from dataFrame\n",
        "df_train.drop(index_neutrals_train , inplace=True)\n",
        "df_test.drop(index_neutrals_test , inplace=True)\n",
        "\n",
        "df_train.loc[(df_train.overall == 1),'overall']= 1\n",
        "df_train.loc[(df_train.overall == 2),'overall']= 1\n",
        "df_train.loc[(df_train.overall == 4),'overall']= 5\n",
        "df_train.loc[(df_train.overall == 5),'overall']= 5\n",
        "\n",
        "df_test.loc[(df_test.overall == 1),'overall']= 1\n",
        "df_test.loc[(df_test.overall == 2),'overall']= 1\n",
        "df_test.loc[(df_test.overall == 4),'overall']= 5\n",
        "df_test.loc[(df_test.overall == 5),'overall']= 5\n",
        "\n",
        "df_train['reviewText_len'].describe()\n",
        "# Since the mean average review size is around 145 chars and max is 400, I can safely set the max [summary + review] Text Limit to 400 \n",
        "\t"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    80000.000000\n",
              "mean       148.534925\n",
              "std        109.884928\n",
              "min          1.000000\n",
              "25%         49.000000\n",
              "50%        129.000000\n",
              "75%        228.000000\n",
              "max        399.000000\n",
              "Name: reviewText_len, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7AXCZXYduHp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "518cd737-b188-4a6f-9733-356542222b63"
      },
      "source": [
        "df_train['overall'].describe()\n",
        "df_test['overall'].describe()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    8000.000000\n",
              "mean        3.000000\n",
              "std         2.000125\n",
              "min         1.000000\n",
              "25%         1.000000\n",
              "50%         3.000000\n",
              "75%         5.000000\n",
              "max         5.000000\n",
              "Name: overall, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13adMhDFoQAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_VOCAB_SIZE = 50000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 200\n",
        "\n",
        "import uuid\n",
        "folderGUID = uuid.uuid4().hex\n",
        "\n",
        "# stupid shell way of converting variable to string \n",
        "!mkdir \"$folderGUID\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGBHgCmFr496",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDpJYm15Psh8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "59d59784-6975-4cb0-d20c-3f2cdf8bfb87"
      },
      "source": [
        "tokenizer = Tokenizer(num_words= MAX_VOCAB_SIZE, filters='#$%&()*+<=>@[\\\\]^_`{|}~\\t\\n', lower=True)\n",
        "tokenizer.fit_on_texts(df_train['reviewText'])\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 28112 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pjaMEuHVXnR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "078f82fe-c3ab-4f38-801c-20cf3aa6c8dd"
      },
      "source": [
        "X = tokenizer.texts_to_sequences(df_train['reviewText'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X.shape)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (80000, 250)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jng1UISwsXxd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "61dd8f87-35de-4af2-bc2e-c9b7eacf2d46"
      },
      "source": [
        "Y = pd.get_dummies(df_train['overall']).values\n",
        "print('Shape of label tensor:', Y.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of label tensor: (80000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzu3hiE2strx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "f2cc599f-3ea9-4d7c-b8a3-99a36c2f62fc"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_VOCAB_SIZE, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "#model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(200))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# run for small number of epochs then save \n",
        "epochs = 10\n",
        "\n",
        "history = model.fit(X, Y, epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2500/2500 [==============================] - 1884s 753ms/step - loss: 0.2660 - accuracy: 0.8935\n",
            "Epoch 2/10\n",
            "2500/2500 [==============================] - 1886s 755ms/step - loss: 0.1629 - accuracy: 0.9400\n",
            "Epoch 3/10\n",
            "2500/2500 [==============================] - 1876s 750ms/step - loss: 0.1184 - accuracy: 0.9577\n",
            "Epoch 4/10\n",
            "2500/2500 [==============================] - 1884s 753ms/step - loss: 0.0863 - accuracy: 0.9702\n",
            "Epoch 5/10\n",
            "2500/2500 [==============================] - 1877s 751ms/step - loss: 0.0658 - accuracy: 0.9774\n",
            "Epoch 6/10\n",
            "2500/2500 [==============================] - 1882s 753ms/step - loss: 0.0502 - accuracy: 0.9833\n",
            "Epoch 7/10\n",
            "2500/2500 [==============================] - 1874s 750ms/step - loss: 0.0409 - accuracy: 0.9866\n",
            "Epoch 8/10\n",
            "2500/2500 [==============================] - 1885s 754ms/step - loss: 0.0336 - accuracy: 0.9891\n",
            "Epoch 9/10\n",
            "2500/2500 [==============================] - 1903s 761ms/step - loss: 0.0273 - accuracy: 0.9916\n",
            "Epoch 10/10\n",
            " 717/2500 [=======>......................] - ETA: 22:55 - loss: 0.0198 - accuracy: 0.9941"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5adje0Av6KyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the model to Local Disk \n",
        "# https://stackoverflow.com/questions/45424683/how-to-continue-training-for-a-saved-and-then-loaded-keras-model\n",
        "\n",
        "filePath = \"ModelResults/\"+str(folderGUID)+\"/model.h5\"\n",
        "model.save(filePath)\n",
        "print(\"Saved model to disk : \"+ str(folderGUID))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaHU2gjz1TZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run additional training if necessary & remember to resave it \n",
        "\n",
        "# Load the model\n",
        "model = load_model(filePath)\n",
        "\n",
        "# Train more on the loaded model\n",
        "model.fit(X, Y, epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x0o4hHdHzxe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "00db75de-711b-4e70-bdfe-eb59094a6c04"
      },
      "source": [
        "\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(df_test['reviewText'].values)\n",
        "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X_test.shape)\n",
        "\n",
        "Y_Test = pd.get_dummies(df_test['overall']).values\n",
        "print('Shape of label tensor:', Y_Test.shape)\n",
        "\n",
        "accr = model.evaluate(X_test,Y_Test)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-64886f7e44b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reviewText'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Shape of data tensor:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rkg_d9COvrvJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "e40e8afe-11e7-46fa-d81d-e45d00a9c591"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "Y_Test_norm = le.fit(Y_Test)\n",
        "\n",
        "y_pred = model.predict(X_test, batch_size=64, verbose=1)\n",
        "y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "\n",
        "print(classification_report(Y_Test_norm, y_pred_bool))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-121-405067756585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mY_Test_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_Test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0man\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mof\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \"\"\"\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bad input shape {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: bad input shape (800, 2)"
          ]
        }
      ]
    }
  ]
}