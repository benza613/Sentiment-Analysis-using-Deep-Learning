{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LexicalAnalysis.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM1wJbbRLHVXAHghfhsk+L6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benza613/Sentiment-Analysis-using-Deep-Learning/blob/master/notebooks/LexicalAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKiUZAYzkhFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive # import drive from google colab\n",
        "\n",
        "ROOT = \"/content/gdrive\"     # default location for the drive\n",
        "print(ROOT)                 # print content of ROOT (Optional)\n",
        "\n",
        "drive.mount(ROOT)           # we mount the google drive at /content/drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PM3s6Zpk1ML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd Sentiment-Analysis-using-Deep-Learning/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ0834Yold8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git pull https://7519148727b530a7734e756bada859a5811db262@github.com/benza613/Sentiment-Analysis-using-Deep-Learning.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU1juLVBv92f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git remote set-url origin https://7519148727b530a7734e756bada859a5811db262@github.com/benza613/Sentiment-Analysis-using-Deep-Learning.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHN_e2Mhzyk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git push origin master"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcfeSCxZx2KG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone github repository setup\n",
        "# import join used to join ROOT path and MY_GOOGLE_DRIVE_PATH\n",
        "from os.path import join  \n",
        "\n",
        "# path to your project on Google Drive\n",
        "MY_GOOGLE_DRIVE_PATH = 'My Drive/SNLP' \n",
        "# replace with your Github username \n",
        "GIT_USERNAME = \"benza613\" \n",
        "# definitely replace with your\n",
        "GIT_TOKEN = \"7519148727b530a7734e756bada859a5811db262\"  \n",
        "# Replace with your github repository in this case we want \n",
        "# to clone deep-learning-v2-pytorch repository\n",
        "GIT_REPOSITORY = \"Sentiment-Analysis-using-Deep-Learning\" \n",
        "\n",
        "PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n",
        "\n",
        "# It's good to print out the value if you are not sure \n",
        "print(\"PROJECT_PATH: \", PROJECT_PATH)   \n",
        "\n",
        "# In case we haven't created the folder already; we will create a folder in the project path \n",
        "!mkdir \"{PROJECT_PATH}\"    \n",
        "\n",
        "#GIT_PATH = \"https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/{GIT_REPOSITORY}.git\" this return 400 Bad Request for me\n",
        "GIT_PATH = \"https://\" + GIT_TOKEN + \"@github.com/\" + GIT_USERNAME + \"/\" + GIT_REPOSITORY + \".git\"\n",
        "print(\"GIT_PATH: \", GIT_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1CoOjkpwURl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  !git config --global user.email \"rjayar4@uic.edu\"\n",
        "  !git config --global user.name \"rakshitharj\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95AcOXwgl51c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#unzip the train and test data\n",
        "from zipfile import ZipFile\n",
        "# Create a ZipFile Object and load abc.zip in it\n",
        "#with ZipFile('Train_20000_Apr-03-2020_23-22.zip', 'r') as zipObj:\n",
        "with ZipFile('Test_20000_Apr-03-2020_23-22.zip', 'r') as zipObj:\n",
        "# Extract all the contents of zip file in same directory\n",
        "  zipObj.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A1iiOa4qzcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "#the preprocessed files are stored in Data_Balanced_20000_Apr-03-2020_23-22 folder \n",
        "#https://github.com/abdulfatir/twitter-sentiment-analysis/blob/master/code/preprocess.py\n",
        "TRAIN_PROCESSED_FILE = 'Data_Balanced_20000_Apr-03-2020_23-22/Train_20000_Apr-03-2020_23-22.csv'\n",
        "TEST_PROCESSED_FILE = 'Data_Balanced_20000_Apr-03-2020_23-22/Test_20000_Apr-03-2020_23-22.csv'\n",
        "\n",
        "#the text files with positive and negative words in data_pos_neg folder\n",
        "POSITIVE_WORDS_FILE = 'data_pos_neg/positive_words.txt'\n",
        "NEGATIVE_WORDS_FILE = 'data_pos_neg/negative_words.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Eg1pQZUsTLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN = True\n",
        "\n",
        "def save_results_to_csv(results, csv_file):\n",
        "    ''' Save list of type [(reviewerID, prediction)] to csv in Kaggle format '''\n",
        "    with open(csv_file, 'w') as csv:\n",
        "        csv.write('id,prediction\\n')\n",
        "        for reviewerID, pred in results:\n",
        "            csv.write(reviewerID)\n",
        "            csv.write(',')\n",
        "            csv.write(str(pred))\n",
        "            csv.write('\\n')\n",
        "\n",
        "def file_to_wordset(filename):\n",
        "    ''' Converts a file with a word per line to a Python set '''\n",
        "    words = []\n",
        "    with open(filename, 'r') as f:\n",
        "        for line in f:\n",
        "            words.append(line.strip())\n",
        "    return set(words)\n",
        "\n",
        "def classify(processed_csv, test_file=True, **params):\n",
        "    positive_words = file_to_wordset(params.pop('positive_words'))\n",
        "    print(\"positive\")\n",
        "    negative_words = file_to_wordset(params.pop('negative_words'))\n",
        "    predictions = []\n",
        "    with open(processed_csv, 'r') as csv:\n",
        "        for line in csv:\n",
        "           # print(\"line\",line)\n",
        "            if test_file:\n",
        "                overall, reviewTime1, reviewTime2, reviewerID,asin,reviewText,*rest = line.strip().split(',')\n",
        "            else:\n",
        "               # print(line.strip().split(','))\n",
        "                overall, reviewTime1, reviewTime2, reviewerID, asin, reviewText,*rest = line.strip().split(',')\n",
        "            pos_count, neg_count = 0, 0\n",
        "            for word in reviewText.split():\n",
        "                #print(\"word\",word)\n",
        "                if word in positive_words:\n",
        "                    pos_count += 1\n",
        "                elif word in negative_words:\n",
        "                    neg_count += 1\n",
        "            # print pos_count, neg_count\n",
        "            prediction = 1 if pos_count >= neg_count else 0\n",
        "            if test_file:\n",
        "                predictions.append((reviewerID, prediction))\n",
        "            else:\n",
        "                predictions.append((reviewerID, prediction))\n",
        "            #print(\"predictions\",predictions)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if TRAIN:\n",
        "        predictions = classify(TRAIN_PROCESSED_FILE, test_file=(not TRAIN), positive_words=POSITIVE_WORDS_FILE, negative_words=NEGATIVE_WORDS_FILE)\n",
        "        #print(predictions)\n",
        "        save_results_to_csv(predictions, 'train_baseline.csv')\n",
        "        #correct = sum([1 for p in predictions if p[1] == 0]) * 100.0 / len(predictions)\n",
        "        #print('Correct = %.2f%%' % correct)\n",
        "    else:\n",
        "        predictions = classify(TEST_PROCESSED_FILE, test_file=(not TRAIN), positive_words=POSITIVE_WORDS_FILE, negative_words=NEGATIVE_WORDS_FILE)\n",
        "        save_results_to_csv(predictions, 'test_baseline.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}