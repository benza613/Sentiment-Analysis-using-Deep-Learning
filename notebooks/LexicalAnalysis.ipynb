{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LexicalAnalysis.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benza613/Sentiment-Analysis-using-Deep-Learning/blob/master/notebooks/LexicalAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKiUZAYzkhFj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive # import drive from google colab\n",
        "\n",
        "ROOT = \"/content/gdrive\"     # default location for the drive\n",
        "print(ROOT)                 # print content of ROOT (Optional)\n",
        "\n",
        "drive.mount(ROOT)           # we mount the google drive at /content/drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PM3s6Zpk1ML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd gdrive/My\\ Drive/SNLP"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8M0wyleB23I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd Sentiment-Analysis-using-Deep-Learning/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A1iiOa4qzcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "#the preprocessed files are stored in Data_Balanced_20000_Apr-03-2020_23-22 folder \n",
        "#https://github.com/abdulfatir/twitter-sentiment-analysis/blob/master/code/preprocess.py\n",
        "\n",
        "#the text files with positive and negative words in data_pos_neg folder\n",
        "POSITIVE_WORDS_FILE = 'data_pos_neg/positive_words.txt'\n",
        "NEGATIVE_WORDS_FILE = 'data_pos_neg/negative_words.txt'\n",
        "\n",
        "\n",
        "Train_ZipCSVFileName = 'Train_100000_Apr-19-2020_22-33.csv'\n",
        "Test_ZipCSVFileName = 'Test_2000_Apr-19-2020_22-40.csv'\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "df_train = pd.read_csv(Train_ZipCSVFileName)\n",
        "df_train.info()\n",
        "\n",
        "df_test = pd.read_csv(Test_ZipCSVFileName)\n",
        "df_test.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Eg1pQZUsTLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN = False\n",
        "\n",
        "def save_results_to_csv(results, csv_file):\n",
        "    ''' Save list of type [(reviewerID, prediction,overall_new)] to csv in Kaggle format '''\n",
        "    with open(csv_file, 'w') as csv:\n",
        "        csv.write('id,prediction,overall_new\\n')\n",
        "        for reviewerID, pred, overall in results:\n",
        "            csv.write(reviewerID)\n",
        "            csv.write(',')\n",
        "            csv.write(str(pred))\n",
        "            csv.write(',')\n",
        "            csv.write(str(overall))\n",
        "            csv.write('\\n')\n",
        "\n",
        "def file_to_wordset(filename):\n",
        "    ''' Converts a file with a word per line to a Python set '''\n",
        "    words = []\n",
        "    with open(filename, 'r') as f:\n",
        "        for line in f:\n",
        "            words.append(line.strip())\n",
        "    return set(words)\n",
        "\n",
        "def classify(processed_csv, test_file=True, **params):\n",
        "    positive_words = file_to_wordset(params.pop('positive_words'))\n",
        "    print(\"positive\")\n",
        "    negative_words = file_to_wordset(params.pop('negative_words'))\n",
        "    predictions = []\n",
        "    overall_new = 0\n",
        "    with open(processed_csv, 'r') as csv:\n",
        "        for line in csv:\n",
        "           # print(\"line\",line)\n",
        "            if test_file:\n",
        "                overall, reviewTime1, reviewTime2, reviewerID,asin,reviewText,*rest = line.strip().split(',')\n",
        "            else:\n",
        "               # print(line.strip().split(','))\n",
        "                overall, reviewTime1, reviewTime2, reviewerID, asin, reviewText,*rest = line.strip().split(',')\n",
        "            pos_count, neg_count = 0, 0\n",
        "            for word in reviewText.split():\n",
        "                #print(\"word\",word)\n",
        "                if word in positive_words:\n",
        "                    pos_count += 1\n",
        "                elif word in negative_words:\n",
        "                    neg_count += 1\n",
        "            # print pos_count, neg_count\n",
        "            #print(\"overall\",type(overall)\n",
        "                if overall == \"5.0\":\n",
        "                    overall_new = 1\n",
        "\n",
        "                if overall == \"4.0\":\n",
        "                    overall_new = 1\n",
        "\n",
        "                if overall == \"3.0\":\n",
        "                    overall_new = 1\n",
        "\n",
        "                if overall == \"1.0\":\n",
        "                    overall_new = 0\n",
        "\n",
        "                if overall == \"2.0\":\n",
        "                    overall_new = 0\n",
        "\n",
        "            prediction = 1 if pos_count >= neg_count else 0\n",
        "           \n",
        "            if test_file:\n",
        "                predictions.append((reviewerID, prediction, overall_new))\n",
        "            else:\n",
        "                predictions.append((reviewerID, prediction, overall_new))\n",
        "            #print(\"predictions\",predictions)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if TRAIN:\n",
        "        predictions = classify(Train_ZipCSVFileName, test_file=(not TRAIN), positive_words=POSITIVE_WORDS_FILE, negative_words=NEGATIVE_WORDS_FILE)\n",
        "        print(predictions)\n",
        "        save_results_to_csv(predictions, 'train_baseline.csv')\n",
        "        correct = sum([1 for p in predictions if p[1] == p[2]]) * 100.0 / len(predictions)\n",
        "        print('Correct = %.2f%%' % correct)\n",
        "    else:\n",
        "        predictions = classify(Test_ZipCSVFileName, test_file=(not TRAIN), positive_words=POSITIVE_WORDS_FILE, negative_words=NEGATIVE_WORDS_FILE)\n",
        "        correct = sum([1 for p in predictions if p[1] == p[2]]) * 100.0 / len(predictions)\n",
        "        print('Correct = %.2f%%' % correct)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}