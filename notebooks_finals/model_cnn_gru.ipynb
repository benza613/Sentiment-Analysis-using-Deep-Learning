{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "model_lstm.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGPgMSbNPpOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a notebook for Cnn-gru prediction model \n",
        "\n",
        "# 1. Load the chosen dataset and try to see the vocab size \n",
        "# 2. Determine a MAX_VOCAB_SIZE incase you observe a vocab dict that is too large (choose the top/most frequent MAX_VOCAB_SIZE entries / Curse of dimensionality)\n",
        "# 3. Determine a MAX_SEQUENCE LENGTH to vectorize for each review (Note: From splitter usually reviews will be limited to 400 + summary )\n",
        "\n",
        "# DO NOT EDIT/ DELETE THIS BLOCK;\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoSwNmKToBLT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "610da4ad-a086-4a62-b38f-a238cf30f866"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qYW78oFoQnP",
        "colab_type": "code",
        "outputId": "53014ee1-d7fd-46cb-856d-4941b2c19c36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# set working directory -> you must set the path into which you have uploaded the zipped file\n",
        "# this is required in the case of colab or local \n",
        "%cd /content/drive/My\\ Drive/SNLP\\ Project\n",
        "# %cd Source/repos/Sentiment-Analysis-using-Deep-Learning"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/SNLP Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q-BzQJ4obkb",
        "colab_type": "code",
        "outputId": "25e4b953-dea1-4652-a6fb-f637262321f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "# list content of drive - verify you are where you are supposed to be\n",
        "%ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \u001b[0m\u001b[01;34mamz_all_beauty\u001b[0m/                    \u001b[01;34mLexical_analysis\u001b[0m/\n",
            " \u001b[01;34mamz_all_electronics\u001b[0m/               meta_lstm_small_200dim.tsv\n",
            " CleantData_Apr-03-2020_01-31.zip   \u001b[01;34mModelResults\u001b[0m/\n",
            " dataset_dumps.json                 \u001b[01;34mModelVisualization\u001b[0m/\n",
            " Electronics_5.json.gz             'Project Ideas.gdoc'\n",
            " \u001b[01;34mFinalModelResults\u001b[0m/                'Project Proposal.gdoc'\n",
            "\u001b[01;34m'Final Report'\u001b[0m/                    \u001b[01;34m'Report MetaData'\u001b[0m/\n",
            " glove.6B.200d.txt                 'Sentiment Analysis - Deep Learning.pptx'\n",
            " glove.6B.zip                       vecs_lstm_small_200dim.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGjRLSETDcM1",
        "colab_type": "code",
        "outputId": "501a2f91-83f0-49eb-99d1-9f4adad405c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "source": [
        "# load your choice of dataset here . Specified paths as folder_datestring/file_datestring.zip\n",
        "import json\n",
        "\n",
        "with open('dataset_dumps.json') as f:\n",
        "  paths_datasets = json.load(f)\n",
        "\n",
        "print(json.dumps(paths_datasets, indent = 4))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"electronics\": {\n",
            "        \"smallTrain\": \"amz_all_electronics/Data_Balanced_2000_Apr-03-2020_06-46/Train_2000_Apr-03-2020_06-46.zip\",\n",
            "        \"smallTest\": \"amz_all_electronics/Data_Balanced_2000_Apr-03-2020_06-46/Test_2000_Apr-03-2020_06-46.zip\",\n",
            "        \"medTrain\": \"amz_all_electronics/Data_Balanced_20000_Apr-03-2020_06-52/Train_20000_Apr-03-2020_06-52.zip\",\n",
            "        \"medTest\": \"amz_all_electronics/Data_Balanced_20000_Apr-03-2020_06-52/Test_20000_Apr-03-2020_06-52.zip\",\n",
            "        \"largeTrain\": \"\",\n",
            "        \"largeTest\": \"\"\n",
            "    },\n",
            "    \"all_beauty\": {\n",
            "        \"smallTrain\": \"\",\n",
            "        \"smallTest\": \"\",\n",
            "        \"medTrain\": \"\",\n",
            "        \"medTest\": \"\",\n",
            "        \"largeTrain\": \"\",\n",
            "        \"largeTest\": \"\"\n",
            "    }\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9Kz7amjoljp",
        "colab_type": "code",
        "outputId": "ae8aae42-4e1c-4ff9-8c71-1a3bc88e7208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        }
      },
      "source": [
        "\n",
        "Train_ZipCSVFileName = paths_datasets['electronics']['smallTrain']\n",
        "Test_ZipCSVFileName = paths_datasets['electronics']['smallTest']\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "df_train = pd.read_csv(Train_ZipCSVFileName)\n",
        "df_train.info()\n",
        "\n",
        "df_test = pd.read_csv(Test_ZipCSVFileName)\n",
        "df_test.info()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 9 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   overall         10000 non-null  float64\n",
            " 1   reviewTime      10000 non-null  object \n",
            " 2   reviewerID      10000 non-null  object \n",
            " 3   asin            10000 non-null  object \n",
            " 4   reviewText      10000 non-null  object \n",
            " 5   summary         10000 non-null  object \n",
            " 6   unixReviewTime  10000 non-null  int64  \n",
            " 7   reviewText_len  10000 non-null  int64  \n",
            " 8   summary_len     10000 non-null  int64  \n",
            "dtypes: float64(1), int64(3), object(5)\n",
            "memory usage: 703.2+ KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 9 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   overall         1000 non-null   float64\n",
            " 1   reviewTime      1000 non-null   object \n",
            " 2   reviewerID      1000 non-null   object \n",
            " 3   asin            1000 non-null   object \n",
            " 4   reviewText      1000 non-null   object \n",
            " 5   summary         1000 non-null   object \n",
            " 6   unixReviewTime  1000 non-null   int64  \n",
            " 7   reviewText_len  1000 non-null   int64  \n",
            " 8   summary_len     1000 non-null   int64  \n",
            "dtypes: float64(1), int64(3), object(5)\n",
            "memory usage: 70.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEPUIWc2pYRT",
        "colab_type": "code",
        "outputId": "76139f47-18da-4bac-a94e-3ca98577ddcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "\n",
        "# Get names of indexes for which column Age has value 30\n",
        "index_neutrals_train = df_train[ df_train['overall'] == 3 ].index\n",
        "index_neutrals_test = df_test[ df_test['overall'] == 3 ].index\n",
        " \n",
        "# Delete these row indexes from dataFrame\n",
        "df_train.drop(index_neutrals_train , inplace=True)\n",
        "df_test.drop(index_neutrals_test , inplace=True)\n",
        "\n",
        "df_train.loc[(df_train.overall == 1),'overall']= 1\n",
        "df_train.loc[(df_train.overall == 2),'overall']= 1\n",
        "df_train.loc[(df_train.overall == 4),'overall']= 5\n",
        "df_train.loc[(df_train.overall == 5),'overall']= 5\n",
        "\n",
        "df_test.loc[(df_test.overall == 1),'overall']= 1\n",
        "df_test.loc[(df_test.overall == 2),'overall']= 1\n",
        "df_test.loc[(df_test.overall == 4),'overall']= 5\n",
        "df_test.loc[(df_test.overall == 5),'overall']= 5\n",
        "\n",
        "df_train['reviewText_len'].describe()\n",
        "# Since the mean average review size is around 145 chars and max is 400, I can safely set the max [summary + review] Text Limit to 400 \n",
        "\t"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    8000.000000\n",
              "mean      146.234875\n",
              "std       109.121888\n",
              "min         1.000000\n",
              "25%        47.000000\n",
              "50%       127.000000\n",
              "75%       223.250000\n",
              "max       399.000000\n",
              "Name: reviewText_len, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7AXCZXYduHp",
        "colab_type": "code",
        "outputId": "385268e7-71d1-4cf6-ae29-02e44d862a01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "df_train['overall'].describe()\n",
        "df_test['summary_len'].describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    8000.000000\n",
              "mean       14.786125\n",
              "std         7.736512\n",
              "min         2.000000\n",
              "25%         9.000000\n",
              "50%        11.000000\n",
              "75%        20.000000\n",
              "max        34.000000\n",
              "Name: summary_len, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13adMhDFoQAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_VOCAB_SIZE = 50000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 200\n",
        "\n",
        "import uuid\n",
        "folderGUID = uuid.uuid4().hex\n",
        "\n",
        "# stupid shell way of converting variable to string \n",
        "#!mkdir \"ModelResults/v3_$folderGUID\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGBHgCmFr496",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDpJYm15Psh8",
        "colab_type": "code",
        "outputId": "a33b3878-593c-49c9-b222-7caaa00e88b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer = Tokenizer(num_words= MAX_VOCAB_SIZE, filters='#$%&()*+<=>@[\\\\]^_`{|}~\\t\\n', lower=True)\n",
        "tokenizer.fit_on_texts(df_train['summary'] + ' DELIM '+df_train['reviewText'])\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8218 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN0qwi2Hn57s",
        "colab_type": "code",
        "outputId": "c645e3a6-1682-41c3-9971-9076bf2fee3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "\n",
        "print(df_train['summary'].values + ' DELIM '+ df_train['reviewText'].values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['one star DELIM volum button were dead on arriv be awar of possibl defect'\n",
            " 'plug fall out DELIM work great when i tape up the plug to the port i see whi this was for sale other than that it work fine'\n",
            " 'do not buy DELIM did not work im sure the seller knew this befor he ship it out date and cannot be use at all those are word from the manufactur'\n",
            " ... 'five star DELIM the number of star say it all'\n",
            " 'five star DELIM good product thank'\n",
            " 'five star DELIM yet anoth awesom tripp lite product i put this into a rack case with a power amplifi']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pjaMEuHVXnR",
        "colab_type": "code",
        "outputId": "eae47518-66ff-478e-9bae-2631c43d052f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X = tokenizer.texts_to_sequences(df_train['summary'].values + ' DELIM '+ df_train['reviewText'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X.shape)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (8000, 250)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jng1UISwsXxd",
        "colab_type": "code",
        "outputId": "279e89ed-8aa3-4f7f-c33e-c9a9ebf6cb18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "Y = pd.get_dummies(df_train['overall']).values\n",
        "print('Shape of label tensor:', Y.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of label tensor: (8000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzu3hiE2strx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dropout, GRU, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_VOCAB_SIZE, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "model.add(Conv1D(32,kernel_size=3,padding='same',activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=3))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Conv1D(64,kernel_size=3,padding='same',activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=3))\n",
        "model.add(Dropout(0.35))\n",
        "model.add(Conv1D(128,kernel_size=3,padding='same',activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=3))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(GRU(100,return_sequences=True))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(2,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# run for small number of epochs then save \n",
        "epochs = 12\n",
        "\n",
        "history = model.fit(X, Y, epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5adje0Av6KyR",
        "colab_type": "code",
        "outputId": "ffba0bb2-04cf-42cd-a2b8-b3a69d50f65e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Save the model to Local Disk \n",
        "# https://stackoverflow.com/questions/45424683/how-to-continue-training-for-a-saved-and-then-loaded-keras-model\n",
        "\n",
        "\n",
        "# folderGUID = 'v3_1abe9af33fd14c8793d580bf99885182'\n",
        "filePath = \"ModelResults/\"+str(folderGUID)+\"/model.h5\"\n",
        "model.save(filePath)\n",
        "print(\"Saved model to disk : \"+ str(folderGUID))\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk : 1abe9af33fd14c8793d580bf99885182\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaHU2gjz1TZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run additional training if necessary & remember to resave it \n",
        "from tensorflow.keras.models import load_model\n",
        "filePath = 'ModelResults/v3_1abe9af33fd14c8793d580bf99885182/model.h5'\n",
        "# Load the model\n",
        "model = load_model(filePath)\n",
        "\n",
        "# Train more on the loaded model\n",
        "#history = model.fit(X, Y, epochs=5)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3x0o4hHdHzxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "X_test = tokenizer.texts_to_sequences(df_test['summary'].values + ' DELIM '+ df_test['reviewText'].values)\n",
        "X_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X_test.shape)\n",
        "\n",
        "Y_Test = pd.get_dummies(df_test['overall']).values\n",
        "print('Shape of label tensor:', Y_Test.shape)\n",
        "\n",
        "accr = model.evaluate(X_test,Y_Test)\n",
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rkg_d9COvrvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.title('Loss')\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.legend()\n",
        "plt.show();\n",
        "\n",
        "plt.title('Accuracy')\n",
        "plt.plot(history.history['accuracy'], label='test')\n",
        "plt.legend()\n",
        "plt.show();\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1kpF-BFG47h",
        "colab_type": "code",
        "outputId": "56cc0c80-6cd0-4317-c14a-b2adae0ef57f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_labels = df_test['overall'].values\n",
        "\n",
        "#all precision\n",
        "y_pred_softmax = model.predict(X_test)\n",
        "y_pred = list()\n",
        "\n",
        "# 1.0 for neg classes( argmax 0) and 5.0 for pos class (argmax 1)\n",
        "# its just for verbosity \n",
        "for local_pred in y_pred_softmax:\n",
        "  y_pred.append(1.0 if np.argmax(local_pred) == 0 else 5.0)\n",
        "\n",
        "print(classification_report(y_labels, y_pred, target_names=['Classified Neg','Classified Pos']))\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                precision    recall  f1-score   support\n",
            "\n",
            "Classified Neg       0.90      0.95      0.93       400\n",
            "Classified Pos       0.95      0.90      0.92       400\n",
            "\n",
            "      accuracy                           0.92       800\n",
            "     macro avg       0.92      0.92      0.92       800\n",
            "  weighted avg       0.92      0.92      0.92       800\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwZ1ZKCVjYKJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        },
        "outputId": "e343c630-14de-426e-cadf-4bc3170280a3"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 250, 200)          10000000  \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 250, 32)           19232     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1 (None, 83, 32)            0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 83, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_7 (Conv1D)            (None, 83, 64)            6208      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_7 (MaxPooling1 (None, 27, 64)            0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 27, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_8 (Conv1D)            (None, 27, 128)           24704     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_8 (MaxPooling1 (None, 9, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 9, 128)            0         \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (None, 9, 100)            69000     \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 9, 100)            0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 900)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2)                 1802      \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 2)                 6         \n",
            "=================================================================\n",
            "Total params: 10,120,952\n",
            "Trainable params: 10,120,952\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}